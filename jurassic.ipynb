{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8436b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import stpsf\n",
    "import os\n",
    "import sep\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve_fft\n",
    "from joblib import Parallel, delayed\n",
    "from astropy.stats import sigma_clipped_stats, sigma_clip\n",
    "from photutils.detection import StarFinder\n",
    "from photutils.aperture import CircularAperture, aperture_photometry\n",
    "from lacosmic.core import lacosmic\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "np.set_printoptions(legacy='1.25') # my environment is a bit wonky :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe66281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fitting(coords,cube,n_int,n_group):\n",
    "    \"\"\"\n",
    "    fits ramps piecewise with 2 straight lines if a jump is detected,\n",
    "    otherwise fits a single line. If a piece is too short, fits only the longer one.\n",
    "    \"\"\"\n",
    "    row, col = coords\n",
    "\n",
    "    grads = [] # straight line gradient based on first data points\n",
    "    intercepts = [] # straight line intercept based on first data points\n",
    "    resids = [] # residuals from curve_fit of power law\n",
    "\n",
    "    for int_num in range(n_int):\n",
    "        x_dat = [i + (int_num)*n_group for i in range(n_group)]\n",
    "        y_dat = [cube[x][row][col] for x in x_dat]\n",
    "        x = np.asarray(x_dat, dtype=float) \n",
    "        y = np.asarray(y_dat, dtype=float)\n",
    "\n",
    "        # removing first and last frames\n",
    "        y[0] = np.nan\n",
    "        y[-1] = np.nan\n",
    "\n",
    "        mask = ~np.isnan(y)\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        p,r,*_ = np.polyfit(x,y,1,full=True)\n",
    "        m = p[0]\n",
    "        c = p[1]\n",
    "        grads.append(m)\n",
    "        intercepts.append(c)\n",
    "        resids.append(r[0] if len(r) > 0 else np.nan)\n",
    "        \n",
    "    return [row, col, grads, intercepts, resids]\n",
    "\n",
    "\n",
    "def run_lacosmic(frame_data, mask):\n",
    "    \"\"\"\n",
    "    running lacosmic so it can be done parallely on frames\n",
    "    mask: boolean mask where True = science pixel, False = bad pixel\n",
    "    \"\"\"\n",
    "    masked_arr = np.where(mask, frame_data, np.nan)\n",
    "    _, _, std = sigma_clipped_stats(masked_arr)\n",
    "    error_arr = np.full(frame_data.shape, std)\n",
    "    lacosmic_mask = ~mask # (True = masked/bad pixel)\n",
    "    data_clean = np.nan_to_num(frame_data, nan=0.0, posinf=0.0, neginf=0.0) # replace nans which lacosmic doesn't like\n",
    "\n",
    "    clean, crmask = lacosmic(data_clean,contrast=4,cr_threshold=2,\n",
    "                             neighbor_threshold=0.9,mask=lacosmic_mask,error=error_arr)\n",
    "\n",
    "    return clean, crmask\n",
    "\n",
    "def _run_sep(frame, data, kernel, mask, save, obs_dir):\n",
    "    \"\"\"\n",
    "    run source extractor in parallel\n",
    "    \"\"\"\n",
    "    # data will be 2d frame - should parallelise in the future (options for customisation below)\n",
    "    # bkg = sep.Background(data)\n",
    "    data = np.array(data, dtype=np.float32, copy=True)\n",
    "    mask = np.array(mask, dtype=bool, copy=True)\n",
    "    bkg = sep.Background(data, mask=~mask)#, bw=64, bh=64, fw=3, fh=3) # honestly confused about which way round the mask is (I think True = masked)\n",
    "    # subtract the background\n",
    "    data_sub = data - bkg\n",
    "\n",
    "    # object detection\n",
    "    objects = sep.extract(data_sub, 1.5, filter_kernel=kernel, err=bkg.globalrms, mask=~mask)\n",
    "    obj_df = pd.DataFrame(objects)\n",
    "\n",
    "    if len(obj_df) == 0:\n",
    "        # return empty filtered_df with same cols\n",
    "        filtered_df = pd.DataFrame(columns=obj_df.columns.tolist() + ['symmetry', 'frame', 'sep_flux', 'sep_fluxerr', 'sep_flag'])\n",
    "        return obj_df, filtered_df\n",
    "\n",
    "    # adding needed cols\n",
    "    obj_df['symmetry'] = (obj_df['a']/obj_df['b']).abs() - 1\n",
    "    obj_df['frame'] = frame\n",
    "\n",
    "    # aperture photometry\n",
    "    flux, fluxerr, flag = sep.sum_circle(data_sub, obj_df['x'], obj_df['y'], 3.0, err=bkg.globalrms, gain=1.0)\n",
    "    obj_df['sep_flux'] = flux\n",
    "    obj_df['sep_fluxerr'] = fluxerr\n",
    "    obj_df['sep_flag'] = flag\n",
    "\n",
    "    # apply filtering\n",
    "    filter_mask = ((obj_df['symmetry'] < 1.5) &\n",
    "                    obj_df['x'].between(16, 1016) &\n",
    "                    obj_df['y'].between(12, 1012))\n",
    "    filtered_df = obj_df[filter_mask]\n",
    "\n",
    "    # plotting\n",
    "    if save and len(filtered_df) > 0:\n",
    "        from matplotlib.patches import Ellipse\n",
    "        matplotlib.use(\"Agg\") # don't show em\n",
    "        fig, ax = plt.subplots()\n",
    "        m, s = np.mean(data_sub), np.std(data_sub)\n",
    "        ax.imshow(data_sub, interpolation='nearest', vmin=m-s, vmax=m+s, origin='lower')\n",
    "        ax.set_title(f\"Frame {frame}\")\n",
    "\n",
    "        for i in filtered_df.index:\n",
    "            e = Ellipse(\n",
    "                xy=(filtered_df.at[i, 'x'], filtered_df.at[i, 'y']),\n",
    "                width=6*filtered_df.at[i, 'a'],\n",
    "                height=6*filtered_df.at[i, 'b'],\n",
    "                angle=filtered_df.at[i, 'theta']*180./np.pi\n",
    "            )\n",
    "            e.set_facecolor('none')\n",
    "            e.set_edgecolor('red')\n",
    "            ax.add_artist(e)\n",
    "\n",
    "        sep_dir = os.path.join(obs_dir, \"sep_frames\")\n",
    "        os.makedirs(sep_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(sep_dir, f\"frame_{frame:03d}.png\"), bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    return obj_df, filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6aa3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jurassic():\n",
    "    \"\"\"\n",
    "        Class for searching the ramps of full array MIRI images for fast transients\n",
    "\n",
    "        JURASSIC: JWST Up the Ramp Analysis Searching the Sky for Infrared Transients\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,file=None,run=True,ramps=True,images=True,significance=True):\n",
    "        \"\"\"\n",
    "        Initialise or whatevs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : str\n",
    "                File name of the observation\n",
    "        \n",
    "\n",
    "        other stuff I guess - will update at some point\n",
    "        \"\"\"\n",
    "\n",
    "        self.file = file\n",
    "        # The filter stuff ends up crashing the code - need to update\n",
    "        self.psf_fwhm_px = { # taken from JDOX\n",
    "            \"F560W\": 1.882,\n",
    "            \"F770W\": 2.445,\n",
    "            \"F1000W\": 2.982,\n",
    "            \"F1130W\": 3.409,\n",
    "            \"F1280W\": 3.818,\n",
    "            \"F1500W\": 4.436,\n",
    "            \"F1800W\": 5.373,\n",
    "            \"F2100W\": 6.127,\n",
    "            \"F2550W\": 7.300,\n",
    "        }\n",
    "        \n",
    "        if run:\n",
    "            self._assign_data()\n",
    "            self._make_cubes()\n",
    "            self._mask_pixels()\n",
    "\n",
    "            if ramps: # search on ramp level\n",
    "                self.parallel_fit_df(self.rampy_cube) # only fitting rampy_cube not mega\n",
    "                self.worst_fits(self.rampy_cube) # atm has 25 worst fitting pixels\n",
    "        \n",
    "            if images or significance: # search on image level\n",
    "                self.mega_inator(self.rampy_cube)\n",
    "                self._cube_gradient(self.mega_cube_masked)\n",
    "                self._reference_frame()\n",
    "                self._cube_differenced()\n",
    "                self._remove_cosmic(self.diff_cube)\n",
    "                self._psf_kernel()\n",
    "                # self.source_finding()\n",
    "                self.source_extracting()\n",
    "                # self._counts_to_df()\n",
    "\n",
    "            if significance:\n",
    "                self._cube_significance()\n",
    "                self._cube_threshold() \n",
    "                self._cube_rolling_sum()\n",
    "                self._significance_output()\n",
    "\n",
    "\n",
    "    def _assign_data(self):\n",
    "        \"\"\"\n",
    "        Opens the fits file and assigns the data to the class\n",
    "        \"\"\"\n",
    "        with fits.open(self.file) as hdul:\n",
    "            self.data = hdul[1].data # science data\n",
    "            self.dq_2d_arr = hdul[2].data # data quality flag array for whole cube\n",
    "            self.dq_3d_arr = hdul[3].data # data quality flag array for each group\n",
    "            self.filter = hdul[0].header['FILTER']\n",
    "            self.filename = hdul[0].header['FILENAME']\n",
    "            self.targname = hdul[0].header['TARGNAME']\n",
    "\n",
    "        # base outputs folder (cwd)\n",
    "        self.base_dir = os.path.join(os.getcwd(), 'outputs')\n",
    "        os.makedirs(self.base_dir, exist_ok=True)\n",
    "        # Remove the filename suffix\n",
    "        suffix = '_mirimage_ramp.fits'\n",
    "        if self.filename.endswith(suffix):\n",
    "            obs_name = self.filename[:-len(suffix)]\n",
    "        else:\n",
    "            obs_name = self.filename  # fallback\n",
    "\n",
    "        obs_name = f\"sep_{obs_name}\" # for the sep runs\n",
    "\n",
    "        # directory for specific observation/segment\n",
    "        self.obs_dir = os.path.join(self.base_dir, obs_name)\n",
    "        os.makedirs(self.obs_dir, exist_ok=True)\n",
    "\n",
    "        self.n_int = len(self.data)\n",
    "        self.n_group = len(self.data[0])\n",
    "        self.n_frame = self.n_int * self.n_group\n",
    "        self.frames = list(range(self.n_frame)) #list of all frame indices\n",
    "\n",
    "        bad_frames = []\n",
    "        for integration in list(range(self.n_int)):\n",
    "            bad_frames.append(integration*self.n_group)\n",
    "            bad_frames.append(((integration+1)*self.n_group)-1)\n",
    "        self.bad_frames = bad_frames\n",
    "\n",
    "        # again the filter stuff that crashes things\n",
    "        try:\n",
    "            self.fwhm = self.psf_fwhm_px[self.filter]\n",
    "        except:\n",
    "            print(f'Unable to find FWHM of filter {self.filter}') # need to use this to make the \n",
    "\n",
    "\n",
    "    def _make_cubes(self):\n",
    "        \"\"\"\n",
    "        makes cube from 4d uncal file, also jump detected cube\n",
    "        \"\"\"\n",
    "        # make the rampy science data cube\n",
    "        ramps = np.array_split(self.data,self.n_int,axis=0)\n",
    "        rampy_cube = np.concatenate(ramps,axis=1)\n",
    "        self.rampy_cube = np.squeeze(rampy_cube)\n",
    "\n",
    "        # make reference cube for jumps detected with calwebb_detector1\n",
    "        dq_ints = np.array_split(self.dq_3d_arr,len(self.dq_3d_arr),axis=0)\n",
    "        dq_cube = np.concatenate(dq_ints,axis=1)\n",
    "        self.dq_cube = np.squeeze(dq_cube) # bitwise cube with all the dq flags\n",
    "\n",
    "        flag = 4 # jump detected flag\n",
    "        jump_cube = np.full(self.dq_cube.shape, False, dtype=bool)\n",
    "\n",
    "        for frame in range(len(self.dq_cube)):\n",
    "            jump_arr = (self.dq_cube[frame] & flag) == flag\n",
    "            jump_cube[frame] = jump_arr\n",
    "\n",
    "        self.jump_cube = jump_cube # a boolean cube where True is for jumps detected\n",
    "        \n",
    "\n",
    "    def _circle_app(self,rad):\n",
    "        \"\"\"\n",
    "        Makes a kinda circular aperture, probably not worth using. - from ryan\n",
    "        \"\"\"\n",
    "        mask = np.zeros((int(rad*2+.5)+1, int(rad*2+.5)+1))\n",
    "        c = rad\n",
    "        x,y = np.where(mask==0)\n",
    "        dist = np.sqrt((x-c)**2 + (y-c)**2)\n",
    "\n",
    "        ind = (dist) < rad + .2\n",
    "        mask[y[ind],x[ind]] = 1\n",
    "\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def _mask_pixels(self,threshold = 47000): # could be udated w/ quality flags from JWST\n",
    "        \"\"\"\n",
    "        returns a list of tuples that are pixel (row,col) coordinates\n",
    "        that have masked out the non-science and saturated pixels\n",
    "        \"\"\"\n",
    "        # load general miri mask\n",
    "        mask = np.load('full_MIRI_mask.npy') # for full array\n",
    "\n",
    "        # mask out pixels that get counts above threshold\n",
    "        mask_sat = self.rampy_cube[-1] < threshold\n",
    "        mask_sat = mask_sat.astype(int) # to convolve with aperture\n",
    "\n",
    "        kernel = self._circle_app(5)\n",
    "\n",
    "        mask_sat = convolve_fft(mask_sat, kernel)\n",
    "        mask_sat = mask_sat >= 0.99 # boolean\n",
    "\n",
    "        # creating a list of tuples which are the (row,column) coords of each science pixel\n",
    "        rows = list(range(self.rampy_cube.shape[1]))\n",
    "        cols = list(range(self.rampy_cube.shape[2]))\n",
    "\n",
    "        pixels = []\n",
    "\n",
    "        for i in rows:\n",
    "                row_num = [i] * len(cols)\n",
    "                pixel_row = list(zip(row_num,cols)) # tuples of a single row's (i's) pixel coordinates\n",
    "                pixels.extend(pixel_row)\n",
    "\n",
    "        self.mask_tot = mask_sat & mask\n",
    "        nan_mask = self.mask_tot * 1.0 \n",
    "        nan_mask[nan_mask < 1] = np.nan\n",
    "        self.nan_mask = nan_mask\n",
    "\n",
    "        pixel_mask = self.mask_tot.flatten(order='C').tolist() # flattening mask to make same size/dimensions as the list of pixel coords\n",
    "        self.masked_pixels = [pixel for pixel, m in zip(pixels, pixel_mask) if m]\n",
    "\n",
    "\n",
    "    def _pixel_integration(self,cube,int_num,row,col):\n",
    "        \"\"\"\n",
    "        Gets the x and y data of a specified integration for a specific pixel in a specified cube\n",
    "        \"\"\"\n",
    "        integration_length = list(range(0,self.n_group))\n",
    "        x_dat = [i + (int_num)*self.n_group for i in integration_length]\n",
    "    \n",
    "        ramp = []\n",
    "        for x in x_dat:\n",
    "            ramp.append(cube[x][row][col])\n",
    "\n",
    "        return x_dat, ramp   \n",
    "\n",
    "\n",
    "    def parallel_fit_df(self,cube,save_df=False,num_cores=40):\n",
    "        \"\"\"\n",
    "        fits all ramps of specified cube parallely\n",
    "        \"\"\"\n",
    "        fitting = Parallel(n_jobs=num_cores, verbose=0)(\n",
    "            delayed(linear_fitting)(pixel,cube,self.n_int,self.n_group) for pixel in self.masked_pixels)\n",
    "\n",
    "        obj_df = pd.DataFrame(fitting, columns=[\"row\",\"col\",\"gradients\",\"intercepts\",\"residuals\"])\n",
    "        obj_df['max_residual'] = obj_df['residuals'].apply(max)\n",
    "\n",
    "        self.obj_df = obj_df\n",
    "        if save_df:\n",
    "            filepath = os.path.join(self.obs_dir, 'ramp_fittings.csv')\n",
    "            self.obj_df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "    def _line(self,m,c,x):\n",
    "        \"\"\"\n",
    "        straight line eqn\n",
    "        \"\"\"\n",
    "        return [i*m + c for i in x]\n",
    "\n",
    "\n",
    "    def _check_jump(self,coords):\n",
    "        \"\"\"\n",
    "        checking if jump was detected in the dq cube\n",
    "        \"\"\"\n",
    "        row, col = coords\n",
    "        # check through all frames for jumps  \n",
    "        vals = self.jump_cube[:, row, col]\n",
    "        if vals.any():\n",
    "            return 1, int(np.argmax(vals))  # 1 and first z index\n",
    "        else:\n",
    "            return 0, None\n",
    "\n",
    "\n",
    "    def worst_fits(self, cube, num=15, require_jump=False):\n",
    "        \"\"\"\n",
    "        Get worst fits by max_residual. Can filter based on jump status: (require_jump=True/False) or not\n",
    "        \"\"\"\n",
    "        # add coords, jump, and jump_frame columns to obj_df\n",
    "        self.obj_df['coords'] = self.obj_df.apply(lambda row: (row['row'], row['col']), axis=1)\n",
    "        self.obj_df[['jump', 'jump_frame']] = self.obj_df['coords'].apply(lambda c: pd.Series(self._check_jump(c)))\n",
    "\n",
    "        # sort by residual\n",
    "        self.worst_fits_df = self.obj_df.sort_values(by='max_residual',ascending=False)\n",
    "\n",
    "        # filter by if jump or not\n",
    "        if require_jump is True:\n",
    "            subdf = self.worst_fits_df[self.worst_fits_df['jump'] == 1].head(num).copy()\n",
    "        elif require_jump is False:\n",
    "            subdf = self.worst_fits_df[self.worst_fits_df['jump'] == 0].head(num).copy()\n",
    "        else:\n",
    "            subdf = self.worst_fits_df.head(num).copy()\n",
    "\n",
    "        self.jump_fit_df = subdf\n",
    "\n",
    "        rows = subdf['row'].values \n",
    "        cols = subdf['col'].values\n",
    "        grads = subdf['gradients'].values\n",
    "        cepts = subdf['intercepts'].values\n",
    "        resids = subdf['residuals'].values\n",
    "\n",
    "        plt.figure(constrained_layout=True, figsize=(16, 12))\n",
    "\n",
    "        for i in range(len(cols)):\n",
    "            ax = plt.subplot(5, 3, i + 1)\n",
    "            row, col = rows[i], cols[i]\n",
    "\n",
    "            for int_num in range(self.n_int):\n",
    "                x, y = self._pixel_integration(cube, int_num, row, col)\n",
    "                x, y = np.asarray(x, float), np.asarray(y, float)\n",
    "\n",
    "                # remove first and last points\n",
    "                x[[0, -1]] = np.nan\n",
    "                y[[0, -1]] = np.nan\n",
    "\n",
    "                mask = np.isfinite(y)\n",
    "                x, y = x[mask], y[mask]\n",
    "\n",
    "                # plot the data\n",
    "                ax.scatter(x, y, c='tab:blue', s=5)\n",
    "\n",
    "                # line fit\n",
    "                m, c, r = grads[i][int_num], cepts[i][int_num], resids[i][int_num]\n",
    "                y_line = self._line(m, c, x)\n",
    "                ax.plot(x, y_line, label=f'r={r:.2f}')\n",
    "\n",
    "            ax.set_title(f'Pixel ({row},{col}) | jump={subdf.iloc[i]['jump']}, frame={subdf.iloc[i]['jump_frame']}')\n",
    "            ax.set_xlabel('Frame')\n",
    "            ax.set_ylabel('Counts')\n",
    "            ax.legend()\n",
    "\n",
    "        plt.suptitle('Bad pixel ramps')\n",
    "        filepath = os.path.join(self.obs_dir, 'worst_fits.png')\n",
    "        plt.savefig(filepath, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --------------------- Image Search -----------------------\n",
    "\n",
    "\n",
    "    def mega_inator(self,cube):\n",
    "        \"\"\"\n",
    "        makes a mega cube out of a rampy one\n",
    "        \"\"\"\n",
    "        frames = list(range(0,self.n_frame))\n",
    "        integrations = list(range(0,self.n_int))\n",
    "        int_frames = [i*self.n_group  for i in integrations]\n",
    "\n",
    "        int_frames.pop(0) # in order to have list of indices of first frames in ramps except for the very first one\n",
    "        end_frames = [((i+1)*self.n_group)-1 for i in integrations] # want to remove the frames with these indices\n",
    "\n",
    "        mega_cube = np.zeros((self.n_frame,cube.shape[1],cube.shape[2]))\n",
    "        difference = 0 # difference between end of one ramp and start of next\n",
    "        zero_ff = 0 # fudge factor to zero the ramps\n",
    "\n",
    "        for frame in frames:\n",
    "            if frame == 0:\n",
    "                zero_ff = cube[frame]\n",
    "                mega_cube[frame] = cube[frame] - zero_ff\n",
    "            elif frame in int_frames:\n",
    "                zero_ff = cube[frame] \n",
    "                difference = mega_cube[frame-2] + 2*(mega_cube[frame-2] - mega_cube[frame-3])\n",
    "                mega_cube[frame] = cube[frame] + difference - zero_ff\n",
    "            else:\n",
    "                mega_cube[frame] = cube[frame] + difference - zero_ff\n",
    "\n",
    "        # recalc int_frames\n",
    "        int_frames = [i * self.n_group for i in integrations]\n",
    "        end_frames = [(i+1) * self.n_group -1 for i in integrations]\n",
    "        int_frames.extend(end_frames)\n",
    "\n",
    "        mega_cube_masked = mega_cube.copy()\n",
    "        nans_frame = np.full_like(mega_cube_masked[0], np.nan)\n",
    "\n",
    "        for frame in int_frames:\n",
    "            mega_cube_masked[frame] = nans_frame\n",
    "\n",
    "        self.mega_cube = mega_cube\n",
    "        self.mega_cube_masked = mega_cube_masked\n",
    "\n",
    "\n",
    "    def _cube_gradient(self,cube):\n",
    "        \"\"\"\n",
    "        make a gradient cube with the fakey fake frames\n",
    "        \"\"\"\n",
    "        fakeified_cube = cube.copy()\n",
    "        for int_num in range(self.n_int-1): # for all integrations but the last one\n",
    "            val = (int_num+1)*self.n_group\n",
    "            fakeified_cube[val-1] = 2*fakeified_cube[val-2] - fakeified_cube[val-3] # last frame of the integration\n",
    "            fakeified_cube[val] = 3*fakeified_cube[val-2] - 2*fakeified_cube[val-3] # first frame of the next integration\n",
    "\n",
    "        self.fakey_cube = fakeified_cube\n",
    "\n",
    "        self.grad_cube = np.gradient(fakeified_cube,axis=0)\n",
    "\n",
    "\n",
    "    def _reference_frame(self):\n",
    "        \"\"\"\n",
    "        makes a reference frame from median of useful images\n",
    "        \"\"\"\n",
    "        no_nans = np.nansum(self.grad_cube,axis=(1,2)) > 0\n",
    "        no_nans[self.bad_frames] = False\n",
    "\n",
    "        # masking out the all nan frames and the fake frames\n",
    "        good_slices = self.grad_cube.copy()[no_nans]\n",
    "\n",
    "        self.med_frame = np.nanmedian(good_slices,axis=0)\n",
    "\n",
    "\n",
    "    def _cube_differenced(self):\n",
    "        \"\"\"\n",
    "        make a differenced cube from gradient cube using a median frame as reference- now for fakey fake frames - to be done\n",
    "        \"\"\"\n",
    "        diff_cube = self.grad_cube.copy() - self.med_frame[np.newaxis,:,:]\n",
    "        diff_cube[self.bad_frames] = np.nan\n",
    "\n",
    "        self.diff_cube = diff_cube\n",
    "        self.diff_cube_masked = self.diff_cube.copy() * self.mask_tot\n",
    "\n",
    "        filepath = os.path.join(self.obs_dir, \"diff_cube.npy\")\n",
    "        np.save(filepath, self.diff_cube)\n",
    "\n",
    "\n",
    "    def _remove_cosmic(self,cube,num_cores=40):\n",
    "        \"\"\"\n",
    "        uses lacosmic to remove the cosmic rays in each frame\n",
    "        \"\"\"\n",
    "        results = Parallel(n_jobs=num_cores,verbose=0)(delayed(run_lacosmic)(cube[i],self.mask_tot) for i in range(len(cube)))\n",
    "        clean_cube = np.array([r[0] for r in results])\n",
    "        cr_mask_cube = np.array([r[1] for r in results])\n",
    "\n",
    "        self.cr_mask_cube = cr_mask_cube\n",
    "        self.clean_cube = clean_cube\n",
    "\n",
    "\n",
    "    def _make_ref_cr_mask(self):\n",
    "        \"\"\"\n",
    "        makes a cosmic ray mask that is a union of the lacosmic cr_mask\n",
    "        and the JWST pipeline jump detections from the dq array\n",
    "        \"\"\"\n",
    "        ref_cr_mask = np.where(self.cr_mask_cube,self.jump_cube,True)\n",
    "        self.ref_cr_mask = ref_cr_mask\n",
    "        \n",
    "\n",
    "    def _psf_kernel(self,size=10):\n",
    "        \"\"\"\n",
    "        creates kernel based on filter using stpsf\n",
    "        \"\"\"\n",
    "        miri = stpsf.MIRI()\n",
    "        miri.filter = self.filter\n",
    "        psf = miri.calc_psf(fov_pixels=size)\n",
    "        self.kernel = psf[3].data\n",
    "\n",
    "    \n",
    "    def source_extracting(self, save=True, num_cores=40):\n",
    "        \"\"\"\n",
    "        using source extractor (sep) instead of StarFinder\n",
    "        \"\"\"\n",
    "        tasks = (delayed(_run_sep)(frame,self.clean_cube[frame],self.kernel,self.mask_tot,save,self.obs_dir)\n",
    "                                                    for frame in range(self.n_frame))\n",
    "\n",
    "        # run in parallel\n",
    "        results = Parallel(n_jobs=num_cores, prefer=\"processes\")(tasks)\n",
    "        obj_dfs, filtered_dfs = zip(*results)\n",
    "\n",
    "        # combine all frames, ignore empty dfs\n",
    "        self.total_df = pd.concat([df for df in obj_dfs if len(df) > 0], ignore_index=True)\n",
    "        self.filtered_sep_df = pd.concat([df for df in filtered_dfs if len(df) > 0], ignore_index=True)\n",
    "\n",
    "        # save as csvs\n",
    "        filepath = os.path.join(self.obs_dir, \"filtered_sources.csv\")\n",
    "        self.filtered_sep_df.to_csv(filepath, index=False)\n",
    "\n",
    "        filepath = os.path.join(self.obs_dir, \"all_sources.csv\")\n",
    "        self.total_df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "# --------------------- Significance Functions -----------------------\n",
    "\n",
    "\n",
    "    def _cube_significance(self,magic_number=3):\n",
    "        \"\"\"\n",
    "        making a significance cube - dividing the differenced cube by the \n",
    "        standard deviation of the background of each frame\n",
    "        \"\"\"\n",
    "        frame_num = list(range(0, self.n_frame))\n",
    "        sig_cube = np.zeros_like(self.diff_cube)\n",
    "\n",
    "        dat = np.where(self.mask_tot[None, :, :], self.clean_cube, np.nan)\n",
    "\n",
    "        for frame in frame_num:\n",
    "            data = dat[frame].copy()\n",
    "            _, med, std = sigma_clipped_stats(data.copy())\n",
    "\n",
    "            sig_cube[frame] = (dat[frame]-med) / std\n",
    "\n",
    "        # compare with the cr ref mask and set to zero where mask is\n",
    "        sig_cube[self.cr_mask_cube] = 0\n",
    "        \n",
    "        self.sig_cube = sig_cube\n",
    "        self.bool_sig_cube = sig_cube > magic_number\n",
    "\n",
    "\n",
    "    def _cube_threshold(self,rad=2,threshold=9):\n",
    "        \"\"\"\n",
    "        convolves the significance cube with a circle and identifies\n",
    "        the bits above a threshold, above which should be psf-like sources\n",
    "        and below are cosmic ray junk stuffs (ideally)\n",
    "        \"\"\"\n",
    "        frame_num = list(range(0, self.n_frame))\n",
    "        conv_sig_cube = np.zeros_like(self.sig_cube)\n",
    "        bool_threshold_cube = np.zeros_like(self.sig_cube)\n",
    "\n",
    "        for frame in frame_num:\n",
    "            conv_sig_cube[frame] = convolve_fft(self.bool_sig_cube[frame],self._circle_app(rad),normalize_kernel=False)\n",
    "            bool_threshold_cube[frame] = conv_sig_cube[frame] > threshold\n",
    "\n",
    "        self.conv_sig_cube = conv_sig_cube\n",
    "        self.bool_threshold_cube = bool_threshold_cube\n",
    "\n",
    "\n",
    "    def _cube_rolling_sum(self,num_frames=4,threshold=3):\n",
    "        \"\"\"\n",
    "        rolling sum over (num_frames) frames of threshold cube, cut for >= threshold\n",
    "        \"\"\"\n",
    "        good_frames_cube = np.delete(self.bool_threshold_cube, self.bad_frames, axis=0)\n",
    "        n_good = good_frames_cube.shape[0]\n",
    "        rows = good_frames_cube.shape[1]\n",
    "        cols = good_frames_cube.shape[2]\n",
    "\n",
    "        rolling_sum_cube = np.zeros((n_good,rows,cols), dtype=int)\n",
    "\n",
    "        for frame in range(n_good):\n",
    "            rolling_sum_cube[frame] = np.sum(good_frames_cube[frame:frame+num_frames], axis=0)\n",
    "\n",
    "        # make cut for >= threshold\n",
    "        bool_rolling_sum_cube = rolling_sum_cube >= threshold\n",
    "\n",
    "        # insert NaN frames here to keep cadence\n",
    "        nan_slice = np.full((rows,cols),np.nan)\n",
    "        false_slice = np.full((rows,cols),False)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(self.bad_frames):\n",
    "            if i % 2 == 0:\n",
    "                #insert before itself - ik this doesn't make sense, trust me\n",
    "                rolling_sum_cube = np.insert(rolling_sum_cube, i, nan_slice, axis=0)\n",
    "                bool_rolling_sum_cube = np.insert(bool_rolling_sum_cube, i, false_slice, axis=0)\n",
    "            else:\n",
    "                #insert after itself - I drew a little diagram to work this out\n",
    "                rolling_sum_cube = np.insert(rolling_sum_cube, i, nan_slice, axis=0)\n",
    "                bool_rolling_sum_cube = np.insert(bool_rolling_sum_cube, i, false_slice, axis=0)\n",
    "            i += 1\n",
    "\n",
    "        self.rolling_sum_cube = rolling_sum_cube\n",
    "        self.bool_rolling_sum_cube = bool_rolling_sum_cube\n",
    "\n",
    "    \n",
    "    def _significance_output(self):\n",
    "        \"\"\"\n",
    "        Making the output for the significance way of things\n",
    "        For now makes (and saves?) a dataframe containing the pixel coords and frame\n",
    "        where something has passed the multiple signicance thresholds.\n",
    "        \"\"\"\n",
    "        frames,rows,cols = np.where(self.bool_rolling_sum_cube==True)\n",
    "        data_dict = {'frame': frames,\n",
    "                     'row': rows,\n",
    "                     'column': cols}\n",
    "        \n",
    "        significance_df = pd.DataFrame(data_dict)\n",
    "        self.significance_df = significance_df\n",
    "        \n",
    "        filepath = os.path.join(self.obs_dir, 'significance.csv')\n",
    "        significance_df.to_csv(filepath,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf078c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Jurassic('pipeline_data/Obs/stage1/trappist-1/jw01177007001_03101_00001-seg004_mirimage_ramp.fits')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jurassic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
