{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8436b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import stpsf\n",
    "import os\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve_fft\n",
    "from joblib import Parallel, delayed\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.detection import StarFinder\n",
    "from photutils.aperture import CircularAperture, aperture_photometry\n",
    "from lacosmic.core import lacosmic\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "np.set_printoptions(legacy='1.25') # my environment is a bit wonky :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fe66281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fitting(coords,cube,n_int,n_group):\n",
    "    \"\"\"\n",
    "    fits ramps piecewise with 2 straight lines if a jump is detected,\n",
    "    otherwise fits a single line. If a piece is too short, fits only the longer one.\n",
    "    \"\"\"\n",
    "    row, col = coords\n",
    "\n",
    "    grads = [] # straight line gradient based on first data points\n",
    "    intercepts = [] # straight line intercept based on first data points\n",
    "    resids = [] # residuals from curve_fit of power law\n",
    "\n",
    "    for int_num in range(n_int):\n",
    "        x_dat = [i + (int_num)*n_group for i in range(n_group)]\n",
    "        y_dat = [cube[x][row][col] for x in x_dat]\n",
    "        x = np.asarray(x_dat, dtype=float) \n",
    "        y = np.asarray(y_dat, dtype=float)\n",
    "\n",
    "        # removing first and last frames\n",
    "        y[0] = np.nan\n",
    "        y[-1] = np.nan\n",
    "\n",
    "        mask = ~np.isnan(y)\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        p,r,*_ = np.polyfit(x,y,1,full=True)\n",
    "        m = p[0]\n",
    "        c = p[1]\n",
    "        grads.append(m)\n",
    "        intercepts.append(c)\n",
    "        resids.append(r[0] if len(r) > 0 else np.nan)\n",
    "        \n",
    "    return [row, col, grads, intercepts, resids]\n",
    "\n",
    "\n",
    "def parallel_lacosmic(frame_data, mask):\n",
    "    \"\"\"\n",
    "    running lacosmic so it can be done parallely on frames\n",
    "    mask: boolean mask where True = science pixel, False = bad pixel\n",
    "    \"\"\"\n",
    "    masked_arr = np.where(mask, frame_data, np.nan)\n",
    "    _, _, std = sigma_clipped_stats(masked_arr)\n",
    "    error_arr = np.full(frame_data.shape, std)\n",
    "    lacosmic_mask = ~mask # (True = masked/bad pixel)\n",
    "    data_clean = np.nan_to_num(frame_data, nan=0.0, posinf=0.0, neginf=0.0) # replace nans which lacosmic doesn't like\n",
    "\n",
    "    clean, crmask = lacosmic(data_clean,contrast=4,cr_threshold=2,\n",
    "                             neighbor_threshold=0.9,mask=lacosmic_mask,error=error_arr)\n",
    "\n",
    "    return clean, crmask\n",
    "\n",
    "\n",
    "def process_frame_joblib(frame,data,kernel,source_sigma,masked_pixels,plot,save,obs_dir):\n",
    "    \"\"\"\n",
    "    parallelising the source_finding function\n",
    "    \"\"\"\n",
    "    if np.isnan(data).all():\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    neg_data = -data\n",
    "    combined = []\n",
    "\n",
    "    # positive sources\n",
    "    _, med, std = sigma_clipped_stats(data)\n",
    "    Finder = StarFinder(med + source_sigma * std, kernel)\n",
    "    stars_pos = Finder(data.copy())\n",
    "\n",
    "    # negative sources\n",
    "    _, med, std = sigma_clipped_stats(neg_data.copy())\n",
    "    Finder = StarFinder(med + source_sigma * std, kernel)\n",
    "    stars_neg = Finder(neg_data.copy())\n",
    "\n",
    "    masked_set = set(masked_pixels)\n",
    "\n",
    "    def _make_df(stars_df, sign):\n",
    "        \"\"\"\n",
    "        makes dataframe from data, based on the sign of the sources\n",
    "        \"\"\"\n",
    "        stars_df['frame'] = frame\n",
    "        stars_df['sign'] = sign\n",
    "        stars_df['coords'] = stars_df.apply(lambda r: (int(round(r['ycentroid'])),\n",
    "                                                       int(round(r['xcentroid']))),\n",
    "                                                       axis=1)\n",
    "        stars_df['not_masked'] = ~stars_df['coords'].isin(masked_set)\n",
    "        stars_df['buffer'] = ((stars_df['xcentroid'] > 1016) |\n",
    "                              (stars_df['xcentroid'] < 16) |\n",
    "                              (stars_df['ycentroid'] > 1012) |\n",
    "                              (stars_df['ycentroid'] < 12))\n",
    "        stars_df['good_fwhm'] = (stars_df['fwhm'] > 3) & (stars_df['fwhm'] < 6) # ahhhhhh\n",
    "        return stars_df\n",
    "\n",
    "    if stars_pos is not None and len(stars_pos) > 0:\n",
    "        combined.append(_make_df(stars_pos.to_pandas(), 1))\n",
    "\n",
    "    if stars_neg is not None and len(stars_neg) > 0:\n",
    "        combined.append(_make_df(stars_neg.to_pandas(), -1))\n",
    "\n",
    "    if len(combined) == 0:\n",
    "        combined_df = pd.DataFrame()\n",
    "    else:\n",
    "        combined_df = pd.concat(combined, ignore_index=True)\n",
    "\n",
    "    # plotting and or saving the frames w/ detections\n",
    "    if save:\n",
    "        matplotlib.use(\"Agg\")\n",
    "    if plot or save:\n",
    "        plt.figure()\n",
    "        plt.title(f\"Frame {frame}\")\n",
    "        plt.imshow(data, vmin=-100, vmax=100)\n",
    "        plt.colorbar()\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        if not combined_df.empty:\n",
    "            good = combined_df[(~combined_df[\"not_masked\"]) &\n",
    "                            (~combined_df[\"buffer\"]) &\n",
    "                            (combined_df[\"good_fwhm\"])]\n",
    "            colours = {1: \"r\", -1: \"white\"}\n",
    "            for sign in [1, -1]:\n",
    "                subset = good[good[\"sign\"] == sign]\n",
    "                plt.scatter(subset[\"xcentroid\"], subset[\"ycentroid\"],\n",
    "                            marker=\"*\", edgecolor=colours[sign],\n",
    "                            facecolor=\"none\", s=60)\n",
    "\n",
    "        if save:\n",
    "            sf_dir = os.path.join(obs_dir, \"SF_frames\")\n",
    "            os.makedirs(sf_dir, exist_ok=True)\n",
    "            plt.savefig(os.path.join(sf_dir, f\"frame_{frame:03d}.png\"),bbox_inches=\"tight\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6aa3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jurassic():\n",
    "    \"\"\"\n",
    "        Class for searching the ramps of full array MIRI images for fast transients\n",
    "\n",
    "        JURASSIC: JWST Up the Ramp Analysis Searching the Sky for Infrared Transients\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,file=None,run=True,ramps=True,images=True,significance=True):\n",
    "        \"\"\"\n",
    "        Initialise or whatevs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : str\n",
    "                File name of the observation\n",
    "        \n",
    "\n",
    "        other stuff I guess - will update at some point\n",
    "        \"\"\"\n",
    "\n",
    "        self.file = file\n",
    "        # The filter stuff ends up crashing the code - need to update\n",
    "        # self.psf_fwhm_px = { # taken from JDOX\n",
    "        #     \"F560W\": 1.882,\n",
    "        #     \"F770W\": 2.445,\n",
    "        #     \"F1000W\": 2.982,\n",
    "        #     \"F1130W\": 3.409,\n",
    "        #     \"F1280W\": 3.818,\n",
    "        #     \"F1500W\": 4.436,\n",
    "        #     \"F1800W\": 5.373,\n",
    "        #     \"F2100W\": 6.127,\n",
    "        #     \"F2550W\": 7.300,\n",
    "        # }\n",
    "        \n",
    "        if run:\n",
    "            self._assign_data()\n",
    "            self._make_cubes()\n",
    "            self._mask_pixels()\n",
    "\n",
    "            if ramps: # search on ramp level\n",
    "                self.parallel_fit_df(self.rampy_cube) # only fitting rampy_cube not mega\n",
    "                self.worst_fits(self.rampy_cube) # atm has 25 worst fitting pixels\n",
    "        \n",
    "            if images or significance: # search on image level\n",
    "                self.mega_inator(self.rampy_cube)\n",
    "                self._cube_gradient(self.mega_cube_masked)\n",
    "                self._reference_frame()\n",
    "                self._cube_differenced()\n",
    "                self._remove_cosmic(self.diff_cube)\n",
    "                self._psf_kernel()\n",
    "                self.source_finding()\n",
    "                self._counts_to_df()\n",
    "\n",
    "            if significance:\n",
    "                self._cube_significance()\n",
    "                self._cube_threshold() \n",
    "                self._cube_rolling_sum()\n",
    "                self._significance_output()\n",
    "\n",
    "\n",
    "    def _assign_data(self):\n",
    "        \"\"\"\n",
    "        Opens the fits file and assigns the data to the class\n",
    "        \"\"\"\n",
    "        with fits.open(self.file) as hdul:\n",
    "            self.data = hdul[1].data # science data\n",
    "            self.dq_2d_arr = hdul[2].data # data quality flag array for whole cube\n",
    "            self.dq_3d_arr = hdul[3].data # data quality flag array for each group\n",
    "            self.filter = hdul[0].header['FILTER']\n",
    "            self.obs_id = hdul[0].header['OBS_ID']\n",
    "            self.exposure = hdul[0].header['EXPOSURE']\n",
    "        self.obs_dir = f\"{self.obs_id}_{self.exposure}_myramp\"\n",
    "        os.makedirs(self.obs_dir, exist_ok=True)\n",
    "\n",
    "        self.n_int = len(self.data)\n",
    "        self.n_group = len(self.data[0])\n",
    "        self.n_frame = self.n_int * self.n_group\n",
    "        self.frames = list(range(self.n_frame)) #list of all frame indices\n",
    "\n",
    "        bad_frames = []\n",
    "        for integration in list(range(self.n_int)):\n",
    "            bad_frames.append(integration*self.n_group)\n",
    "            bad_frames.append(((integration+1)*self.n_group)-1)\n",
    "        self.bad_frames = bad_frames\n",
    "\n",
    "        # again the filter stuff that crashes things\n",
    "        # try:\n",
    "        #     self.fwhm = self.psf_fwhm_px[self.filter]\n",
    "        # except:\n",
    "        #     print(f'Unable to find FWHM of filter {self.filter}') # need to use this to make the \n",
    "\n",
    "\n",
    "    def _make_cubes(self):\n",
    "        \"\"\"\n",
    "        makes cube from 4d uncal file, also jump detected cube\n",
    "        \"\"\"\n",
    "        # make the rampy science data cube\n",
    "        ramps = np.array_split(self.data,self.n_int,axis=0)\n",
    "        rampy_cube = np.concatenate(ramps,axis=1)\n",
    "        self.rampy_cube = np.squeeze(rampy_cube)\n",
    "\n",
    "        # make reference cube for jumps detected with calwebb_detector1\n",
    "        dq_ints = np.array_split(self.dq_3d_arr,len(self.dq_3d_arr),axis=0)\n",
    "        dq_cube = np.concatenate(dq_ints,axis=1)\n",
    "        self.dq_cube = np.squeeze(dq_cube) # bitwise cube with all the dq flags\n",
    "\n",
    "        flag = 4 # jump detected flag\n",
    "        jump_cube = np.full(self.dq_cube.shape, False, dtype=bool)\n",
    "\n",
    "        for frame in range(len(self.dq_cube)):\n",
    "            jump_arr = (self.dq_cube[frame] & flag) == flag\n",
    "            jump_cube[frame] = jump_arr\n",
    "\n",
    "        self.jump_cube = jump_cube # a boolean cube where True is for jumps detected\n",
    "        \n",
    "\n",
    "    def _circle_app(self,rad):\n",
    "        \"\"\"\n",
    "        Makes a kinda circular aperture, probably not worth using. - from ryan\n",
    "        \"\"\"\n",
    "        mask = np.zeros((int(rad*2+.5)+1, int(rad*2+.5)+1))\n",
    "        c = rad\n",
    "        x,y = np.where(mask==0)\n",
    "        dist = np.sqrt((x-c)**2 + (y-c)**2)\n",
    "\n",
    "        ind = (dist) < rad + .2\n",
    "        mask[y[ind],x[ind]] = 1\n",
    "\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def _mask_pixels(self,threshold = 47000): # could be udated w/ quality flags from JWST\n",
    "        \"\"\"\n",
    "        returns a list of tuples that are pixel (row,col) coordinates\n",
    "        that have masked out the non-science and saturated pixels\n",
    "        \"\"\"\n",
    "        # load general miri mask\n",
    "        mask = np.load('full_MIRI_mask.npy') # for full array\n",
    "\n",
    "        # mask out pixels that get counts above threshold\n",
    "        mask_sat = self.rampy_cube[-1] < threshold\n",
    "        mask_sat = mask_sat.astype(int) # to convolve with aperture\n",
    "\n",
    "        kernel = self._circle_app(5)\n",
    "\n",
    "        mask_sat = convolve_fft(mask_sat, kernel)\n",
    "        mask_sat = mask_sat >= 0.99 # boolean\n",
    "\n",
    "        # creating a list of tuples which are the (row,column) coords of each science pixel\n",
    "        rows = list(range(self.rampy_cube.shape[1]))\n",
    "        cols = list(range(self.rampy_cube.shape[2]))\n",
    "\n",
    "        pixels = []\n",
    "\n",
    "        for i in rows:\n",
    "                row_num = [i] * len(cols)\n",
    "                pixel_row = list(zip(row_num,cols)) # tuples of a single row's (i's) pixel coordinates\n",
    "                pixels.extend(pixel_row)\n",
    "\n",
    "        self.mask_tot = mask_sat & mask\n",
    "        nan_mask = self.mask_tot * 1.0 \n",
    "        nan_mask[nan_mask < 1] = np.nan\n",
    "        self.nan_mask = nan_mask\n",
    "\n",
    "        pixel_mask = self.mask_tot.flatten(order='C').tolist() # flattening mask to make same size/dimensions as the list of pixel coords\n",
    "        self.masked_pixels = [pixel for pixel, m in zip(pixels, pixel_mask) if m]\n",
    "\n",
    "\n",
    "    def _pixel_integration(self,cube,int_num,row,col):\n",
    "        \"\"\"\n",
    "        Gets the x and y data of a specified integration for a specific pixel in a specified cube\n",
    "        \"\"\"\n",
    "        integration_length = list(range(0,self.n_group))\n",
    "        x_dat = [i + (int_num)*self.n_group for i in integration_length]\n",
    "    \n",
    "        ramp = []\n",
    "        for x in x_dat:\n",
    "            ramp.append(cube[x][row][col])\n",
    "\n",
    "        return x_dat, ramp   \n",
    "\n",
    "\n",
    "    def parallel_fit_df(self,cube,save_df=False,num_cores=40):\n",
    "        \"\"\"\n",
    "        fits all ramps of specified cube parallely\n",
    "        \"\"\"\n",
    "        fitting = Parallel(n_jobs=num_cores, verbose=0)(\n",
    "            delayed(linear_fitting)(pixel,cube,self.n_int,self.n_group) for pixel in self.masked_pixels)\n",
    "\n",
    "        df = pd.DataFrame(fitting, columns=[\"row\",\"col\",\"gradients\",\"intercepts\",\"residuals\"])\n",
    "        df['max_residual'] = df['residuals'].apply(max)\n",
    "\n",
    "        self.df = df\n",
    "        if save_df:\n",
    "            filepath = os.path.join(self.obs_dir, 'ramp_fittings.csv')\n",
    "            self.df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "    def _line(self,m,c,x):\n",
    "        \"\"\"\n",
    "        straight line eqn\n",
    "        \"\"\"\n",
    "        return [i*m + c for i in x]\n",
    "\n",
    "\n",
    "    def _check_jump(self,coords):\n",
    "        \"\"\"\n",
    "        checking if jump was detected in the dq cube\n",
    "        \"\"\"\n",
    "        row, col = coords\n",
    "        # check through all frames for jumps  \n",
    "        vals = self.jump_cube[:, row, col]\n",
    "        if vals.any():\n",
    "            return 1, int(np.argmax(vals))  # 1 and first z index\n",
    "        else:\n",
    "            return 0, None\n",
    "\n",
    "\n",
    "    def worst_fits(self, cube, num=15, require_jump=False):\n",
    "        \"\"\"\n",
    "        Get worst fits by max_residual. Can filter based on jump status: (require_jump=True/False) or not\n",
    "        \"\"\"\n",
    "        # add coords, jump, and jump_frame columns to df\n",
    "        self.df['coords'] = self.df.apply(lambda row: (row['row'], row['col']), axis=1)\n",
    "        self.df[['jump', 'jump_frame']] = self.df['coords'].apply(lambda c: pd.Series(self._check_jump(c)))\n",
    "\n",
    "        # sort by residual\n",
    "        self.worst_fits_df = self.df.sort_values(by='max_residual',ascending=False)\n",
    "\n",
    "        # filter by if jump or not\n",
    "        if require_jump is True:\n",
    "            subdf = self.worst_fits_df[self.worst_fits_df['jump'] == 1].head(num).copy()\n",
    "        elif require_jump is False:\n",
    "            subdf = self.worst_fits_df[self.worst_fits_df['jump'] == 0].head(num).copy()\n",
    "        else:\n",
    "            subdf = self.worst_fits_df.head(num).copy()\n",
    "\n",
    "        self.jump_fit_df = subdf\n",
    "\n",
    "        rows = subdf['row'].values \n",
    "        cols = subdf['col'].values\n",
    "        grads = subdf['gradients'].values\n",
    "        cepts = subdf['intercepts'].values\n",
    "        resids = subdf['residuals'].values\n",
    "\n",
    "        plt.figure(constrained_layout=True, figsize=(16, 12))\n",
    "\n",
    "        for i in range(len(cols)):\n",
    "            ax = plt.subplot(5, 3, i + 1)\n",
    "            row, col = rows[i], cols[i]\n",
    "\n",
    "            for int_num in range(self.n_int):\n",
    "                x, y = self._pixel_integration(cube, int_num, row, col)\n",
    "                x, y = np.asarray(x, float), np.asarray(y, float)\n",
    "\n",
    "                # remove first and last points\n",
    "                x[[0, -1]] = np.nan\n",
    "                y[[0, -1]] = np.nan\n",
    "\n",
    "                mask = np.isfinite(y)\n",
    "                x, y = x[mask], y[mask]\n",
    "\n",
    "                # plot the data\n",
    "                ax.scatter(x, y, c='tab:blue', s=5)\n",
    "\n",
    "                # line fit\n",
    "                m, c, r = grads[i][int_num], cepts[i][int_num], resids[i][int_num]\n",
    "                y_line = self._line(m, c, x)\n",
    "                ax.plot(x, y_line, label=f'r={r:.2f}')\n",
    "\n",
    "            ax.set_title(f'Pixel ({row},{col}) | jump={subdf.iloc[i]['jump']}, frame={subdf.iloc[i]['jump_frame']}')\n",
    "            ax.set_xlabel('Frame')\n",
    "            ax.set_ylabel('Counts')\n",
    "            ax.legend()\n",
    "\n",
    "        plt.suptitle('Bad pixel ramps')\n",
    "        filepath = os.path.join(self.obs_dir, 'worst_fits.png')\n",
    "        plt.savefig(filepath, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --------------------- Image Search -----------------------\n",
    "\n",
    "\n",
    "    def mega_inator(self,cube):\n",
    "        \"\"\"\n",
    "        makes a mega cube out of a rampy one\n",
    "        \"\"\"\n",
    "        frames = list(range(0,self.n_frame))\n",
    "        integrations = list(range(0,self.n_int))\n",
    "        int_frames = [i*self.n_group  for i in integrations]\n",
    "\n",
    "        int_frames.pop(0) # in order to have list of indices of first frames in ramps except for the very first one\n",
    "        end_frames = [((i+1)*self.n_group)-1 for i in integrations] # want to remove the frames with these indices\n",
    "\n",
    "        mega_cube = np.zeros((self.n_frame,cube.shape[1],cube.shape[2]))\n",
    "        difference = 0 # difference between end of one ramp and start of next\n",
    "        zero_ff = 0 # fudge factor to zero the ramps\n",
    "\n",
    "        for frame in frames:\n",
    "            if frame == 0:\n",
    "                zero_ff = cube[frame]\n",
    "                mega_cube[frame] = cube[frame] - zero_ff\n",
    "            elif frame in int_frames:\n",
    "                zero_ff = cube[frame] \n",
    "                difference = mega_cube[frame-2] + 2*(mega_cube[frame-2] - mega_cube[frame-3])\n",
    "                mega_cube[frame] = cube[frame] + difference - zero_ff\n",
    "            else:\n",
    "                mega_cube[frame] = cube[frame] + difference - zero_ff\n",
    "\n",
    "        # recalc int_frames\n",
    "        int_frames = [i * self.n_group for i in integrations]\n",
    "        end_frames = [(i+1) * self.n_group -1 for i in integrations]\n",
    "        int_frames.extend(end_frames)\n",
    "\n",
    "        mega_cube_masked = mega_cube.copy()\n",
    "        nans_frame = np.full_like(mega_cube_masked[0], np.nan)\n",
    "\n",
    "        for frame in int_frames:\n",
    "            mega_cube_masked[frame] = nans_frame\n",
    "\n",
    "        self.mega_cube = mega_cube\n",
    "        self.mega_cube_masked = mega_cube_masked\n",
    "\n",
    "\n",
    "    def _cube_gradient(self,cube):\n",
    "        \"\"\"\n",
    "        make a gradient cube with the fakey fake frames\n",
    "        \"\"\"\n",
    "        fakeified_cube = cube.copy()\n",
    "        for int_num in range(self.n_int-1): # for all integrations but the last one\n",
    "            val = (int_num+1)*self.n_group\n",
    "            fakeified_cube[val-1] = 2*fakeified_cube[val-2] - fakeified_cube[val-3] # last frame of the integration\n",
    "            fakeified_cube[val] = 3*fakeified_cube[val-2] - 2*fakeified_cube[val-3] # first frame of the next integration\n",
    "\n",
    "        self.fakey_cube = fakeified_cube\n",
    "\n",
    "        self.grad_cube = np.gradient(fakeified_cube,axis=0)\n",
    "\n",
    "\n",
    "    def _reference_frame(self):\n",
    "        \"\"\"\n",
    "        makes a reference frame from median of useful images\n",
    "        \"\"\"\n",
    "        no_nans = np.nansum(self.grad_cube,axis=(1,2)) > 0\n",
    "\n",
    "        for frame in self.bad_frames:\n",
    "            no_nans[frame] = False\n",
    "\n",
    "        # masking out the all nan frames and the fake frames\n",
    "        good_slices = self.grad_cube.copy()[no_nans]\n",
    "\n",
    "        self.med_frame = np.nanmedian(good_slices,axis=0)\n",
    "\n",
    "\n",
    "    def _cube_differenced(self): # split up to deal with nan frames\n",
    "        \"\"\"\n",
    "        make a differenced cube from gradient cube using a median frame as reference- now for fakey fake frames - to be done\n",
    "        \"\"\"\n",
    "        diff_cube = np.zeros_like(self.grad_cube)\n",
    "\n",
    "        for frame in range(2,self.n_frame-2): # first integration with 2 nan frames either side\n",
    "            diff_cube[frame] = self.grad_cube[frame] - self.med_frame\n",
    "\n",
    "        for int_num in range(self.n_int): # rest of the integrations with 2 nans at the end?\n",
    "            diff_cube[(int_num+1)*self.n_group-1] = np.zeros_like(self.grad_cube[0])\n",
    "\n",
    "        self.diff_cube = diff_cube\n",
    "        self.diff_cube_masked = self.diff_cube.copy() * self.mask_tot\n",
    "\n",
    "\n",
    "    def _remove_cosmic(self,cube,num_cores=40):\n",
    "        \"\"\"\n",
    "        uses lacosmic to remove the cosmic rays in each frame\n",
    "        \"\"\"\n",
    "        results = Parallel(n_jobs=num_cores,verbose=0)(delayed(parallel_lacosmic)(cube[i],self.mask_tot) for i in range(len(cube)))\n",
    "        clean_cube = np.array([r[0] for r in results])\n",
    "        cr_mask_cube = np.array([r[1] for r in results])\n",
    "\n",
    "        self.cr_mask_cube = cr_mask_cube\n",
    "        self.clean_cube = clean_cube\n",
    "\n",
    "\n",
    "    def _make_ref_cr_mask(self):\n",
    "        \"\"\"\n",
    "        makes a cosmic ray mask that is a union of the lacosmic cr_mask\n",
    "        and the JWST pipeline jump detections from the dq array\n",
    "        \"\"\"\n",
    "        ref_cr_mask = np.where(self.cr_mask_cube,self.jump_cube,True)\n",
    "        self.ref_cr_mask = ref_cr_mask\n",
    "        \n",
    "\n",
    "    def _psf_kernel(self,size=10):\n",
    "        \"\"\"\n",
    "        creates kernel based on filter using stpsf\n",
    "        \"\"\"\n",
    "        miri = stpsf.MIRI()\n",
    "        miri.filter = self.filter\n",
    "        psf = miri.calc_psf(fov_pixels=size)\n",
    "        self.kernel = psf[3].data\n",
    "\n",
    "\n",
    "    def source_finding(self,source_sigma=12,save=True,plot=False,num_cores=40):\n",
    "        \"\"\"\n",
    "        Now in parallel form (ooh ahh)\n",
    "        detecting transients using StarFinder on cleaned differenced cube\n",
    "        skips all-NaN frames -starfinder is not a fan of those\n",
    "        \"\"\"\n",
    "        tasks = (delayed(process_frame_joblib)(frame,self.clean_cube[frame],self.kernel,\n",
    "                                            source_sigma,self.masked_pixels,plot,save,self.obs_dir)\n",
    "                                            for frame in range(self.n_frame))\n",
    "\n",
    "        results = Parallel(n_jobs=num_cores, prefer=\"processes\")(tasks)\n",
    "\n",
    "        # combine all frame results\n",
    "        self.source_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "        filtered = self.source_df[(~self.source_df[\"not_masked\"]) &\n",
    "                                  (~self.source_df[\"buffer\"]) &\n",
    "                                  (self.source_df[\"good_fwhm\"])]\n",
    "\n",
    "        self.filtered_source_df = filtered\n",
    "        filepath = os.path.join(self.obs_dir, \"filtered_sources.csv\")\n",
    "        filtered.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "    def _ap_photometry(self,row):\n",
    "        \"\"\"\n",
    "        aperture photometry on the filtered dataframe of detected sources\n",
    "        \"\"\"\n",
    "        image = self.diff_cube_masked[row['frame']]\n",
    "        aperture_radius = row['fwhm'] * 1.5 # 1.5x fwhm for radius of aperture\n",
    "        annulus_r_in = 4 * aperture_radius \n",
    "        annulus_r_out = 6 * aperture_radius \n",
    "        aperture = CircularAperture( (row['xcentroid'], row['ycentroid']), aperture_radius )\n",
    "        phot_table = aperture_photometry(image, aperture)\n",
    "        phot_table['aperture_sum'][0]\n",
    "\n",
    "        # identify background sample\n",
    "        ys, xs = np.indices(image.shape)\n",
    "        dist = np.hypot(xs - row['xcentroid'], ys - row['ycentroid'])\n",
    "        isannulus = (dist > annulus_r_in) & (dist < annulus_r_out)\n",
    "        bgimage = np.empty(image.shape) + np.nan\n",
    "        bgimage[isannulus] = image[isannulus]\n",
    "        medbg = np.median(image[isannulus])\n",
    "\n",
    "        counts = phot_table['aperture_sum'][0] - medbg * aperture.area\n",
    "        return counts\n",
    "\n",
    "    \n",
    "    def _counts_to_df(self):\n",
    "        \"\"\"\n",
    "        add the counts from aperture photometry to the filtered dataframe\n",
    "        \"\"\"\n",
    "        self.filtered_source_df['counts'] = self.filtered_source_df.apply(self._ap_photometry,axis=1)\n",
    "\n",
    "\n",
    "# --------------------- Significance Functions -----------------------\n",
    "\n",
    "\n",
    "    def _cube_significance(self,magic_number=3):\n",
    "        \"\"\"\n",
    "        making a significance cube - dividing the differenced cube by the \n",
    "        standard deviation of the background of each frame\n",
    "        \"\"\"\n",
    "        frame_num = list(range(0, self.n_frame))\n",
    "        sig_cube = np.zeros_like(self.diff_cube)\n",
    "\n",
    "        dat = np.where(self.mask_tot[None, :, :], self.clean_cube, np.nan)\n",
    "\n",
    "        for frame in frame_num:\n",
    "            data = dat[frame].copy()\n",
    "            _, med, std = sigma_clipped_stats(data.copy())\n",
    "\n",
    "            sig_cube[frame] = (dat[frame]-med) / std\n",
    "\n",
    "        # compare with the cr ref mask and set to zero where mask is\n",
    "        sig_cube[self.cr_mask_cube] = 0\n",
    "        \n",
    "        self.sig_cube = sig_cube\n",
    "        self.bool_sig_cube = sig_cube > magic_number\n",
    "\n",
    "\n",
    "    def _cube_threshold(self,rad=2,threshold=9):\n",
    "        \"\"\"\n",
    "        convolves the significance cube with a circle and identifies\n",
    "        the bits above a threshold, above which should be psf-like sources\n",
    "        and below are cosmic ray junk stuffs (ideally)\n",
    "        \"\"\"\n",
    "        frame_num = list(range(0, self.n_frame))\n",
    "        conv_sig_cube = np.zeros_like(self.sig_cube)\n",
    "        bool_threshold_cube = np.zeros_like(self.sig_cube)\n",
    "\n",
    "        for frame in frame_num:\n",
    "            conv_sig_cube[frame] = convolve_fft(self.bool_sig_cube[frame],self._circle_app(rad),normalize_kernel=False)\n",
    "            bool_threshold_cube[frame] = conv_sig_cube[frame] > threshold\n",
    "\n",
    "        self.conv_sig_cube = conv_sig_cube\n",
    "        self.bool_threshold_cube = bool_threshold_cube\n",
    "\n",
    "\n",
    "    def _cube_rolling_sum(self,num_frames=4,threshold=3):\n",
    "        \"\"\"\n",
    "        rolling sum over (num_frames) frames of threshold cube, cut for >= threshold\n",
    "        \"\"\"\n",
    "        good_frames_cube = np.delete(self.bool_threshold_cube, self.bad_frames, axis=0)\n",
    "        n_good = good_frames_cube.shape[0]\n",
    "        rows = good_frames_cube.shape[1]\n",
    "        cols = good_frames_cube.shape[2]\n",
    "\n",
    "        rolling_sum_cube = np.zeros((n_good,rows,cols), dtype=int)\n",
    "\n",
    "        for frame in range(n_good):\n",
    "            rolling_sum_cube[frame] = np.sum(good_frames_cube[frame:frame+num_frames], axis=0)\n",
    "\n",
    "        # make cut for >= threshold\n",
    "        bool_rolling_sum_cube = rolling_sum_cube >= threshold\n",
    "\n",
    "        # insert NaN frames here to keep cadence\n",
    "        nan_slice = np.full((rows,cols),np.nan)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(self.bad_frames):\n",
    "            if i % 2 == 0:\n",
    "                #insert before itself - ik this doesn't make sense, trust me\n",
    "                rolling_sum_cube = np.insert(rolling_sum_cube, i, nan_slice, axis=0)\n",
    "                bool_rolling_sum_cube = np.insert(bool_rolling_sum_cube, i, nan_slice, axis=0)\n",
    "            else:\n",
    "                #insert after itself - I drew a little diagram to work this out\n",
    "                rolling_sum_cube = np.insert(rolling_sum_cube, i, nan_slice, axis=0)\n",
    "                bool_rolling_sum_cube = np.insert(bool_rolling_sum_cube, i, nan_slice, axis=0)\n",
    "            i += 1\n",
    "\n",
    "        self.rolling_sum_cube = rolling_sum_cube\n",
    "        self.bool_rolling_sum_cube = bool_rolling_sum_cube\n",
    "\n",
    "    \n",
    "    def _significance_output(self):\n",
    "        \"\"\"\n",
    "        Making the output for the significance way of things\n",
    "        For now makes (and saves?) a dataframe containing the pixel coords and frame\n",
    "        where something has passed the multiple signicance thresholds.\n",
    "        \"\"\"\n",
    "        frames,rows,cols = np.where(self.rolling_sum_cube==True)\n",
    "        data_dict = {\n",
    "            'frame': frames,\n",
    "            'row': rows,\n",
    "            'column': cols\n",
    "        }\n",
    "        significance_df = pd.DataFrame(data_dict)\n",
    "        self.significance_df = significance_df\n",
    "        \n",
    "        filepath = os.path.join(self.obs_dir, 'significance.csv')\n",
    "        significance_df.to_csv(filepath,index=False)\n",
    "\n",
    "\n",
    "# --------------------- Plotting Functions -----------------------\n",
    "        \n",
    "\n",
    "    def plot_ramp(self,cube,coords,scatter=None,plot=None,clean=True,save=False):\n",
    "        \"\"\"\n",
    "        plots the ramp of specified pixel in specified cube\n",
    "        \"\"\"\n",
    "        row = coords[0]\n",
    "        col = coords[1]\n",
    "        \n",
    "        plt.figure()\n",
    "        for int_num in range(self.n_int):\n",
    "            x, y = self._pixel_integration(cube, int_num, row, col)\n",
    "            y = np.asarray(y, dtype=float)\n",
    "            x = np.asarray(x, dtype=float)\n",
    "\n",
    "            if clean: # remove first and last points\n",
    "                y[0] = np.nan\n",
    "                y[-1] = np.nan\n",
    "                x[0] = np.nan\n",
    "                x[-1] = np.nan\n",
    "\n",
    "                mask = np.isfinite(y)\n",
    "                x = x[mask]\n",
    "                y = y[mask]\n",
    "\n",
    "            # plot the data\n",
    "            if scatter:\n",
    "                plt.scatter(x, y, c='tab:blue', s=5)\n",
    "            if plot:\n",
    "                plt.plot(x, y, c='tab:blue')\n",
    "        plt.title(f'Ramps for pixel {coords}')\n",
    "        plt.xlabel('Frame')\n",
    "\n",
    "        if save:\n",
    "            plt.savefig('temp_ramp_title.png',dpi=600,bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def plot_image(self,cube,frame,scale=None,save=False):\n",
    "        \"\"\"\n",
    "        just a quicker way to plot the images\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        plt.title(f'Frame {frame}')\n",
    "\n",
    "        if scale:\n",
    "            plt.imshow(cube[frame],vmin=scale[0],vmax=scale[1],origin='lower')\n",
    "        else:\n",
    "            plt.imshow(cube[frame],origin='lower')\n",
    "\n",
    "        plt.colorbar()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig('temp_img_title.png',dpi=600,bbox_inches='tight')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf078c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Jurassic('pipeline_data/Obs/stage1/trappist-1/jw01177009001_03101_00001-seg001_mirimage_ramp.fits')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jurassic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
