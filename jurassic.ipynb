{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8436b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "**WARNING**: LOCAL JWST PRD VERSION PRDOPSSOC-068 DOESN'T MATCH THE CURRENT ONLINE VERSION PRDOPSSOC-071\n",
      "Please consider updating pysiaf, e.g. pip install --upgrade pysiaf or conda update pysiaf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import stpsf\n",
    "\n",
    "# from scipy.optimize import curve_fit\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve_fft\n",
    "from joblib import Parallel, delayed\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.detection import StarFinder\n",
    "# from photutils.detection import DAOStarFinder\n",
    "from photutils.aperture import CircularAperture, aperture_photometry\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe66281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fitting(coords,cube,n_int,n_group):\n",
    "    \"\"\"\n",
    "    fits ramps piecewise with 2 straight lines if a jump is detected,\n",
    "    otherwise fits a single line. If a piece is too short, fits only the longer one.\n",
    "    \"\"\"\n",
    "    row, col = coords\n",
    "\n",
    "    grads = [] # straight line gradient based on first data points\n",
    "    intercepts = [] # straight line intercept based on first data points\n",
    "    resids = [] # residuals from curve_fit of power law\n",
    "\n",
    "    for int_num in range(n_int):\n",
    "        x_dat = [i + (int_num)*n_group for i in range(n_group)]\n",
    "        y_dat = [cube[x][row][col] for x in x_dat]\n",
    "        x = np.asarray(x_dat, dtype=float) \n",
    "        y = np.asarray(y_dat, dtype=float)\n",
    "\n",
    "        # removing first and last frames\n",
    "        y[0] = np.nan\n",
    "        y[-1] = np.nan\n",
    "\n",
    "        mask = ~np.isnan(y)\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "\n",
    "        p,r,*_ = np.polyfit(x,y,1,full=True)\n",
    "        m = p[0]\n",
    "        c = p[1]\n",
    "        grads.append(m)\n",
    "        intercepts.append(c)\n",
    "        resids.append(r[0] if len(r) > 0 else np.nan)\n",
    "        \n",
    "    return [row, col, grads, intercepts, resids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jurassic():\n",
    "    \"\"\"\n",
    "        Class for searching the ramps of full array MIRI images for fast transients\n",
    "\n",
    "        JURASSIC: JWST Up the Ramp Analysis Searching the Sky for Infrared Transients\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,file=None,ramps=True,images=True,run=True):\n",
    "        \"\"\"\n",
    "        Initialise or whatevs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : str\n",
    "                File name of the observation\n",
    "        \n",
    "\n",
    "        other stuff I guess\n",
    "        \"\"\"\n",
    "\n",
    "        self.file = file\n",
    "        \n",
    "        if run:\n",
    "            self._assign_data()\n",
    "            self._make_cubes()\n",
    "            self._mask_pixels()\n",
    "\n",
    "            if ramps: # search on ramp level\n",
    "                self.parallel_fit_df(self.rampy_cube) # only fitting rampy_cube not mega\n",
    "                self.worst_fits(self.rampy_cube) # atm has 25 worst fitting pixels\n",
    "        \n",
    "            if images: # search on image level\n",
    "                self.mega_inator(self.rampy_cube)\n",
    "                self._cube_gradient(self.mega_cube_masked)\n",
    "                self._reference_frame()\n",
    "                self._cube_differenced()\n",
    "                self._psf_kernel()\n",
    "                self.source_finding()\n",
    "                self._counts_to_df()\n",
    "\n",
    "                self._cube_significance()\n",
    "                self._cube_threshold() \n",
    "                self._cube_rolling_sum()\n",
    "\n",
    "\n",
    "\n",
    "    def _assign_data(self):\n",
    "        \"\"\"\n",
    "        Opens the fits file and assigns the data to the class\n",
    "        \"\"\"\n",
    "        with fits.open(self.file) as hdul:\n",
    "            self.data = hdul[1].data # science data\n",
    "            self.dq_2d_arr = hdul[2].data # data quality flag array for whole cube\n",
    "            self.dq_3d_arr = hdul[3].data # data quality flag array for each group\n",
    "            self.filter = hdul[0].header['FILTER']\n",
    "\n",
    "        self.n_int = len(self.data)\n",
    "        self.n_group = len(self.data[0])\n",
    "        self.n_frame = self.n_int * self.n_group\n",
    "\n",
    "\n",
    "    def _make_cubes(self):\n",
    "        \"\"\"\n",
    "        makes cube from 4d uncal file, also empty quality cube\n",
    "        \"\"\"\n",
    "        # make the rampy science data cube\n",
    "        ramps = np.array_split(self.data,self.n_int,axis=0)\n",
    "        rampy_cube = np.concatenate(ramps,axis=1)\n",
    "        self.rampy_cube = np.squeeze(rampy_cube)\n",
    "\n",
    "        # make reference cube for jumps detected with calwebb_detector1\n",
    "        dq_ints = np.array_split(self.dq_3d_arr,len(self.dq_3d_arr),axis=0)\n",
    "        dq_cube = np.concatenate(dq_ints,axis=1)\n",
    "        self.dq_cube = np.squeeze(dq_cube) # bitwise cube with all the dq flags\n",
    "\n",
    "        flag = 4 # jump detected flag\n",
    "        jump_cube = np.full(self.dq_cube.shape, False, dtype=bool)\n",
    "\n",
    "        for frame in range(len(self.dq_cube)):\n",
    "            jump_arr = (self.dq_cube[frame] & flag) == flag\n",
    "            jump_cube[frame] = jump_arr\n",
    "\n",
    "        self.jump_cube = jump_cube # a boolean cube where True is for jumps detected\n",
    "        \n",
    "\n",
    "    def _circle_app(self,rad):\n",
    "        \"\"\"\n",
    "        Makes a kinda circular aperture, probably not worth using. - from ryan\n",
    "        \"\"\"\n",
    "        mask = np.zeros((int(rad*2+.5)+1, int(rad*2+.5)+1))\n",
    "        c = rad\n",
    "        x,y = np.where(mask==0)\n",
    "        dist = np.sqrt((x-c)**2 + (y-c)**2)\n",
    "\n",
    "        ind = (dist) < rad + .2\n",
    "        mask[y[ind],x[ind]] = 1\n",
    "\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def _mask_pixels(self,threshold = 47000): # could be udated w/ quality flags\n",
    "        \"\"\"\n",
    "        returns a list of tuples that are pixel (row,col) coordinates\n",
    "        that have masked out the non-science and saturated pixels\n",
    "        \"\"\"\n",
    "        # load general miri mask\n",
    "        mask = np.load('full_MIRI_mask.npy') # for full array\n",
    "\n",
    "        # mask out pixels that get counts above threshold\n",
    "        mask_sat = self.rampy_cube[-1] < threshold\n",
    "        mask_sat = mask_sat.astype(int) # to convolve with aperture\n",
    "\n",
    "        kernel = self._circle_app(5)\n",
    "\n",
    "        mask_sat = convolve_fft(mask_sat, kernel)\n",
    "        mask_sat = mask_sat >= 0.99 # boolean\n",
    "\n",
    "        # creating a list of tuples which are the (row,column) coords of each science pixel\n",
    "        rows = list(range(self.rampy_cube.shape[1]))\n",
    "        cols = list(range(self.rampy_cube.shape[2]))\n",
    "\n",
    "        pixels = []\n",
    "\n",
    "        for i in rows:\n",
    "                row_num = [i] * len(cols)\n",
    "                pixel_row = list(zip(row_num,cols)) # tuples of a single row's (i's) pixel coordinates\n",
    "                pixels.extend(pixel_row)\n",
    "\n",
    "        self.mask_tot = mask_sat & mask\n",
    "        nan_mask = self.mask_tot * 1.0 \n",
    "        nan_mask[nan_mask < 1] = np.nan\n",
    "        self.nan_mask = nan_mask\n",
    "\n",
    "        pixel_mask = self.mask_tot.flatten(order='C').tolist() # flattening mask to make same size/dimensions as the list of pixel coords\n",
    "        self.masked_pixels = [pixel for pixel, m in zip(pixels, pixel_mask) if m]\n",
    "\n",
    "\n",
    "    def _pixel_integration(self,cube,int_num,row,col):\n",
    "        \"\"\"\n",
    "        Gets the x and y data of a specified integration for a specific pixel in a specified cube\n",
    "        \"\"\"\n",
    "        integration_length = list(range(0,self.n_group))\n",
    "        x_dat = [i + (int_num)*self.n_group for i in integration_length]\n",
    "    \n",
    "        ramp = []\n",
    "        for x in x_dat:\n",
    "            ramp.append(cube[x][row][col])\n",
    "\n",
    "        return x_dat, ramp   \n",
    "\n",
    "\n",
    "    def parallel_fit_df(self,cube,save_df=False,num_cores=20):\n",
    "        \"\"\"\n",
    "        fits all ramps of specified cube parallely\n",
    "        \"\"\"\n",
    "        fitting = Parallel(n_jobs=num_cores, verbose=0)(\n",
    "            delayed(linear_fitting)(pixel,cube,self.n_int,self.n_group) for pixel in self.masked_pixels)\n",
    "\n",
    "        df = pd.DataFrame(fitting, columns=[\"row\",\"col\",\"gradients\",\"intercepts\",\"residuals\"])\n",
    "        df['max_residual'] = df['residuals'].apply(max)\n",
    "\n",
    "        self.df = df\n",
    "        if save_df:\n",
    "            self.df.to_csv('ramp fittings.csv', index=False)\n",
    "\n",
    "\n",
    "    def _line(self,m,c,x):\n",
    "        \"\"\"\n",
    "        straight line eqn lol\n",
    "        \"\"\"\n",
    "        return [i*m + c for i in x]\n",
    "\n",
    "\n",
    "    def _check_jump(self,coords):\n",
    "        \"\"\"\n",
    "        checking if jump was detected in the dq cube\n",
    "        \"\"\"\n",
    "        row, col = coords\n",
    "        # check through all frames for jumps  \n",
    "        vals = self.jump_cube[:, row, col]\n",
    "        if vals.any():\n",
    "            return 1, int(np.argmax(vals))  # 1 and first z index\n",
    "        else:\n",
    "            return 0, None\n",
    "\n",
    "\n",
    "    def worst_fits(self, cube, num=15, require_jump=False):\n",
    "        \"\"\"\n",
    "        Get worst fits by max_residual. Option to filter on jump status: (require_jump=True/False)\n",
    "        \"\"\"\n",
    "        # add coords, jump, and jump_frame columns to df\n",
    "        self.df['coords'] = self.df.apply(lambda row: (row['row'], row['col']), axis=1)\n",
    "        self.df[['jump', 'jump_frame']] = self.df['coords'].apply(lambda c: pd.Series(self._check_jump(c)))\n",
    "\n",
    "        # sort by residual\n",
    "        self.worst_fits_df = self.df.sort_values(by='max_residual',ascending=False)\n",
    "\n",
    "        # filter by if jump or not\n",
    "        if require_jump is True:\n",
    "            subdf = self.worst_fits_df[self.worst_fits_df['jump'] == 1].head(num).copy()\n",
    "        elif require_jump is False:\n",
    "            subdf = self.worst_fits_df[self.worst_fits_df['jump'] == 0].head(num).copy()\n",
    "        else:\n",
    "            subdf = self.worst_fits_df.head(num).copy()\n",
    "\n",
    "        self.jump_fit_df = subdf\n",
    "\n",
    "        rows = subdf['row'].values \n",
    "        cols = subdf['col'].values\n",
    "        grads = subdf['gradients'].values\n",
    "        cepts = subdf['intercepts'].values\n",
    "        resids = subdf['residuals'].values\n",
    "\n",
    "        plt.figure(constrained_layout=True, figsize=(16, 12))\n",
    "\n",
    "        for i in range(len(cols)):\n",
    "            ax = plt.subplot(5, 3, i + 1)\n",
    "            row, col = rows[i], cols[i]\n",
    "\n",
    "            for int_num in range(self.n_int):\n",
    "                x, y = self._pixel_integration(cube, int_num, row, col)\n",
    "                x, y = np.asarray(x, float), np.asarray(y, float)\n",
    "\n",
    "                # remove first and last points\n",
    "                x[[0, -1]] = np.nan\n",
    "                y[[0, -1]] = np.nan\n",
    "\n",
    "                mask = np.isfinite(y)\n",
    "                x, y = x[mask], y[mask]\n",
    "\n",
    "                # plot the data\n",
    "                ax.scatter(x, y, c='tab:blue', s=5)\n",
    "\n",
    "                # line fit\n",
    "                m, c, r = grads[i][int_num], cepts[i][int_num], resids[i][int_num]\n",
    "                y_line = self._line(m, c, x)\n",
    "                ax.plot(x, y_line, label=f'r={r:.2f}')\n",
    "\n",
    "            ax.set_title(f'Pixel ({row},{col}) | jump={subdf.iloc[i]['jump']}, frame={subdf.iloc[i]['jump_frame']}')\n",
    "            ax.set_xlabel('Frame')\n",
    "            ax.set_ylabel('Counts')\n",
    "            ax.legend()\n",
    "\n",
    "        plt.suptitle('Bad pixel ramps')\n",
    "        plt.savefig('worst_fits.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --------------------- Image Search -----------------------\n",
    "\n",
    "\n",
    "    def mega_inator(self,cube):\n",
    "        \"\"\"\n",
    "        makes a mega cube out of a rampy one\n",
    "        \"\"\"\n",
    "        frames = list(range(0,self.n_frame))\n",
    "        integrations = list(range(0,self.n_int))\n",
    "        int_frames = [i*self.n_group  for i in integrations]\n",
    "\n",
    "        int_frames.pop(0) # in order to have list of indices of first frames in ramps except for the very first one\n",
    "        end_frames = [((i+1)*self.n_group)-1 for i in integrations] # want to remove the frames with these indices\n",
    "\n",
    "        mega_cube = np.zeros((self.n_frame,cube.shape[1],cube.shape[2]))\n",
    "        difference = 0 # difference between end of one ramp and start of next\n",
    "        zero_ff = 0 # fudge factor to zero the ramps\n",
    "\n",
    "        for frame in frames:\n",
    "            if frame == 0:\n",
    "                zero_ff = cube[frame]\n",
    "                mega_cube[frame] = cube[frame] - zero_ff\n",
    "            elif frame in int_frames:\n",
    "                zero_ff = cube[frame] \n",
    "                difference = mega_cube[frame-2] + 2*(mega_cube[frame-2] - mega_cube[frame-3])\n",
    "                mega_cube[frame] = cube[frame] + difference - zero_ff\n",
    "            else:\n",
    "                mega_cube[frame] = cube[frame] + difference - zero_ff\n",
    "\n",
    "        # recalac int_frames\n",
    "        int_frames = [i * self.n_group for i in integrations]\n",
    "        end_frames = [(i+1) * self.n_group -1 for i in integrations]\n",
    "        int_frames.extend(end_frames)\n",
    "\n",
    "        mega_cube_masked = mega_cube.copy()\n",
    "        nans_frame = np.full_like(mega_cube_masked[0], np.nan)\n",
    "\n",
    "        for frame in int_frames:\n",
    "            mega_cube_masked[frame] = nans_frame\n",
    "\n",
    "        self.mega_cube = mega_cube\n",
    "        self.mega_cube_masked = mega_cube_masked\n",
    "\n",
    "\n",
    "    def _cube_gradient(self,cube):\n",
    "        \"\"\"\n",
    "        make a gradient cube with the fakey fake frames - to be done\n",
    "        \"\"\"\n",
    "        fakeified_cube = cube.copy()\n",
    "        for int_num in range(self.n_int-1): # for all integrations but the last one\n",
    "            val = (int_num+1)*self.n_group\n",
    "            fakeified_cube[val-1] = 2*fakeified_cube[val-2] - fakeified_cube[val-3] # last frame of the integration\n",
    "            fakeified_cube[val] = 3*fakeified_cube[val-2] - 2*fakeified_cube[val-3] # first frame of the next integration\n",
    "\n",
    "        self.fakey_cube = fakeified_cube\n",
    "\n",
    "        self.grad_cube = np.gradient(fakeified_cube,axis=0)\n",
    "\n",
    "\n",
    "    def _reference_frame(self):\n",
    "        \"\"\"\n",
    "        makes a reference frame from median of useful images\n",
    "        \"\"\"\n",
    "        fake_frames = []\n",
    "        for int_num in range(self.n_int-1):\n",
    "            fake_frames.append((int_num+1)*self.n_group-1)\n",
    "\n",
    "        no_nans = np.nansum(self.grad_cube,axis=(1,2)) > 0\n",
    "\n",
    "        for frame in fake_frames:\n",
    "            no_nans[frame] = False\n",
    "\n",
    "        # masking out the all nan frames and the fake frames\n",
    "        good_slices = self.grad_cube.copy()[no_nans]\n",
    "\n",
    "        self.med_frame = np.nanmedian(good_slices,axis=0)\n",
    "\n",
    "\n",
    "    def _cube_differenced(self): # split up to deal with nan frames\n",
    "        \"\"\"\n",
    "        make a differenced cube from gradient cube using a median frame as reference- now for fakey fake frames - to be done\n",
    "        \"\"\"\n",
    "        diff_cube = np.zeros_like(self.grad_cube)\n",
    "\n",
    "        for frame in range(2,self.n_frame-2):\n",
    "            diff_cube[frame] = self.grad_cube[frame] - self.med_frame\n",
    "\n",
    "        for int_num in range(self.n_int):\n",
    "            diff_cube[(int_num+1)*self.n_group-1] = np.zeros_like(self.grad_cube[0])\n",
    "\n",
    "        self.diff_cube = diff_cube\n",
    "        self.diff_cube_masked = self.diff_cube.copy() * self.mask_tot\n",
    "\n",
    "\n",
    "    def _psf_kernel(self,size=10):\n",
    "        \"\"\"\n",
    "        creates kernel based on filter\n",
    "        \"\"\"\n",
    "        miri = stpsf.MIRI()\n",
    "        miri.filter = self.filter\n",
    "        psf = miri.calc_psf(fov_pixels=size)\n",
    "        self.kernel = psf[3].data\n",
    "\n",
    "\n",
    "    def source_finding(self,source_sigma: float = 12,plot=True,cut=True):\n",
    "        \"\"\"\n",
    "        detecting transients using *StarFinder* on differenced cube\n",
    "        skips all-NaN frames\n",
    "        \"\"\"\n",
    "        frame_num = list(range(0, self.n_frame))\n",
    "        self.source_df = pd.DataFrame()\n",
    "\n",
    "        for frame in frame_num:\n",
    "            data = self.diff_cube[frame].copy()\n",
    "            neg_data = (-1)*data\n",
    "            combined_df = pd.DataFrame()\n",
    "\n",
    "            if np.isnan(data).all():\n",
    "                print(f\"Skipping frame {frame}: (oops) all NaNs.\")\n",
    "\n",
    "            _, med, std = sigma_clipped_stats(neg_data.copy())\n",
    "            Finder = StarFinder(med + source_sigma * std, self.kernel)\n",
    "            stars_neg = Finder(neg_data.copy())\n",
    "\n",
    "            _, med, std = sigma_clipped_stats(data.copy())\n",
    "            Finder = StarFinder(med + source_sigma * std, self.kernel)\n",
    "            stars_pos = Finder(data.copy())\n",
    "\n",
    "            if plot:\n",
    "                plt.figure()\n",
    "                plt.title(f'Frame {frame}')\n",
    "                plt.imshow(data, vmin=-100, vmax=100)\n",
    "                plt.colorbar()\n",
    "\n",
    "            masked_set = set(self.masked_pixels)\n",
    "\n",
    "            if stars_pos is not None and len(stars_pos) > 0:\n",
    "                stars_pos_df = stars_pos.to_pandas()\n",
    "                stars_pos_df['frame'] = frame\n",
    "                stars_pos_df['sign'] = 1\n",
    "                stars_pos_df['coords'] = stars_pos_df.apply(lambda row: (int(round(row['ycentroid'])), int(round(row['xcentroid']))), axis=1)\n",
    "                stars_pos_df['not_masked'] = ~stars_pos_df['coords'].isin(masked_set)\n",
    "                stars_pos_df['buffer'] = ((stars_pos_df['xcentroid'] > 1016) | (stars_pos_df['xcentroid'] < 16) |\n",
    "                                            (stars_pos_df['ycentroid'] > 1012) | (stars_pos_df['ycentroid'] < 12))\n",
    "                stars_pos_df['good_fwhm'] = (stars_pos_df['fwhm'] > 3) & (stars_pos_df['fwhm'] < 6) # FHWM cut\n",
    "                combined_df = pd.concat([combined_df, stars_pos_df])\n",
    "\n",
    "                if plot: # plot only sources not in mask region\n",
    "                    if cut:\n",
    "                        masked_pos = stars_pos_df[(~stars_pos_df['not_masked'])&(~stars_pos_df['buffer'])&(stars_pos_df['good_fwhm'])]\n",
    "                        plt.scatter(masked_pos['xcentroid'], masked_pos['ycentroid'],\n",
    "                                   marker='*', edgecolor='r', facecolor='none', s=60, label='+')\n",
    "                    else:\n",
    "                        plt.scatter(stars_pos_df['xcentroid'], stars_pos_df['ycentroid'],\n",
    "                                   marker='*', edgecolor='r', facecolor='none', s=60, label='+')\n",
    "            else:\n",
    "                print(f'No positive sources detected in frame {frame}')\n",
    "\n",
    "            if stars_neg is not None and len(stars_neg) > 0:\n",
    "                stars_neg_df = stars_neg.to_pandas()\n",
    "                stars_neg_df['frame'] = frame\n",
    "                stars_neg_df['sign'] = -1\n",
    "                stars_neg_df['coords'] = stars_neg_df.apply(lambda row: (int(round(row['ycentroid'])), int(round(row['xcentroid']))), axis=1)\n",
    "                stars_neg_df['not_masked'] = ~stars_neg_df['coords'].isin(masked_set)\n",
    "                stars_neg_df['buffer'] = ((stars_neg_df['xcentroid'] > 1016) | (stars_neg_df['xcentroid'] < 16) |\n",
    "                                            (stars_neg_df['ycentroid'] > 1012) | (stars_neg_df['ycentroid'] < 12))\n",
    "                stars_neg_df['good_fwhm'] = (stars_neg_df['fwhm'] > 3) & (stars_neg_df['fwhm'] < 6)\n",
    "                combined_df = pd.concat([combined_df, stars_neg_df])\n",
    "\n",
    "                if plot: # plot only sources not in mask region\n",
    "                    if cut:\n",
    "                        masked_neg = stars_neg_df[(~stars_neg_df['not_masked'])&(~stars_neg_df['buffer'])&(stars_neg_df['good_fwhm'])]\n",
    "                        plt.scatter(masked_neg['xcentroid'], masked_neg['ycentroid'],\n",
    "                                    marker='*', edgecolor='white', facecolor='none', s=60, label='-')\n",
    "                    else:\n",
    "                        plt.scatter(stars_neg_df['xcentroid'], stars_neg_df['ycentroid'],\n",
    "                                    marker='*', edgecolor='white', facecolor='none', s=60, label='-')\n",
    "            else:\n",
    "                print(f'No negative sources detected in frame {frame}')\n",
    "\n",
    "            self.source_df = pd.concat([self.source_df,combined_df])\n",
    "\n",
    "            if plot:\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.legend()\n",
    "\n",
    "        # filtering the detections so have df of sources in science region, not near edge of detector,\n",
    "        # and have a 'good' fhwm, that is greater than 3 and less than 6 currently\n",
    "        filtered_source_df = self.source_df[(~self.source_df['not_masked'])&(~self.source_df['buffer'])&(self.source_df['good_fwhm'])]\n",
    "        self.filtered_source_df = filtered_source_df\n",
    "\n",
    "\n",
    "    def _ap_photometry(self,row):\n",
    "        \"\"\"\n",
    "        aperture photometry on the filtered dataframe of detected sources\n",
    "        \"\"\"\n",
    "        image = self.diff_cube_masked[row['frame']]\n",
    "        aperture_radius = row['fwhm'] * 1.5 # 1.5x fwhm for radius of aperture\n",
    "        annulus_r_in = 4 * aperture_radius \n",
    "        annulus_r_out = 6 * aperture_radius \n",
    "        aperture = CircularAperture( (row['xcentroid'], row['ycentroid']), aperture_radius )\n",
    "        phot_table = aperture_photometry(image, aperture)\n",
    "        phot_table['aperture_sum'][0]\n",
    "\n",
    "\n",
    "        # identify background sample\n",
    "        ys, xs = np.indices(image.shape)\n",
    "        dist = np.hypot(xs - row['xcentroid'], ys - row['ycentroid'])\n",
    "        isannulus = (dist > annulus_r_in) & (dist < annulus_r_out)\n",
    "        bgimage = np.empty(image.shape) + np.nan\n",
    "        bgimage[isannulus] = image[isannulus]\n",
    "        medbg = np.median(image[isannulus])\n",
    "\n",
    "\n",
    "        counts = phot_table['aperture_sum'][0] - medbg * aperture.area\n",
    "        return counts\n",
    "\n",
    "    \n",
    "    def _counts_to_df(self):\n",
    "        \"\"\"\n",
    "        add the counts from aperture photometry to the filtered dataframe\n",
    "        \"\"\"\n",
    "        self.filtered_source_df['counts'] = self.filtered_source_df.apply(self._ap_photometry,axis=1)\n",
    "\n",
    "\n",
    "# --------------------- StarFinder Avoidance Functions -----------------------\n",
    "\n",
    "\n",
    "    def _cube_significance(self,magic_number=3):\n",
    "        \"\"\"\n",
    "        making a significance cube - dividing the differenced cube by the \n",
    "        standard deviation of the background of each frame\n",
    "        \"\"\"\n",
    "        frame_num = list(range(0, self.n_frame))\n",
    "        sig_cube = np.zeros_like(self.diff_cube)\n",
    "\n",
    "        for frame in frame_num:\n",
    "            data = self.diff_cube_masked[frame].copy()\n",
    "            _, med, std = sigma_clipped_stats(data.copy())\n",
    "\n",
    "            sig_cube[frame] = (self.diff_cube_masked[frame]-med) / std\n",
    "        \n",
    "        self.sig_cube = sig_cube\n",
    "        self.bool_sig_cube = sig_cube > magic_number\n",
    "\n",
    "\n",
    "    def _cube_threshold(self,rad=2,threshold=9):\n",
    "        \"\"\"\n",
    "        convolves the significance cube with a circle and identifies\n",
    "        the bits above a threshold, above which should be psf-like sources\n",
    "        and below are cosmic ray junk stuffs\n",
    "        \"\"\"\n",
    "        frame_num = list(range(0, self.n_frame))\n",
    "        conv_sig_cube = np.zeros_like(self.sig_cube)\n",
    "        bool_threshold_cube = np.zeros_like(self.sig_cube)\n",
    "\n",
    "        for frame in frame_num:\n",
    "            conv_sig_cube[frame] = convolve_fft(self.bool_sig_cube[frame],self._circle_app(rad),normalize_kernel=False)\n",
    "\n",
    "            bool_threshold_cube[frame] = conv_sig_cube[frame] > threshold\n",
    "\n",
    "        self.conv_sig_cube = conv_sig_cube\n",
    "        self.bool_threshold_cube = bool_threshold_cube\n",
    "\n",
    "\n",
    "    def _cube_rolling_sum(self,num_frames=4,threshold=3):\n",
    "        \"\"\"\n",
    "        rolling sum over (num_frames) frames of threshold cube, cut for >= threshold\n",
    "        \"\"\"\n",
    "        rolling_sum_cube = np.zeros_like(self.bool_threshold_cube, dtype=int)\n",
    "\n",
    "        for frame in range(self.n_frame):\n",
    "            rolling_sum_cube[frame] = np.sum(self.bool_threshold_cube[frame:frame+num_frames], axis=0)\n",
    "\n",
    "        # make cut for >= threshold\n",
    "        bool_rolling_sum_cube = rolling_sum_cube >= threshold\n",
    "\n",
    "        self.rolling_sum_cube = rolling_sum_cube\n",
    "        self.bool_rolling_sum_cube = bool_rolling_sum_cube\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- Plotting Functions -----------------------\n",
    "        \n",
    "\n",
    "    def plot_ramp(self,cube,coords,scatter=None,plot=None,clean=True,save=False):\n",
    "        \"\"\"\n",
    "        plots the ramp of specified pixel in specified cube\n",
    "        \"\"\"\n",
    "        row = coords[0]\n",
    "        col = coords[1]\n",
    "        \n",
    "        plt.figure()\n",
    "        for int_num in range(self.n_int):\n",
    "            x, y = self._pixel_integration(cube, int_num, row, col)\n",
    "            y = np.asarray(y, dtype=float)\n",
    "            x = np.asarray(x, dtype=float)\n",
    "\n",
    "            if clean: # remove first and last points\n",
    "                y[0] = np.nan\n",
    "                y[-1] = np.nan\n",
    "                x[0] = np.nan\n",
    "                x[-1] = np.nan\n",
    "\n",
    "                mask = np.isfinite(y)\n",
    "                x = x[mask]\n",
    "                y = y[mask]\n",
    "\n",
    "            # plot the data\n",
    "            if scatter:\n",
    "                plt.scatter(x, y, c='tab:blue', s=5)\n",
    "            if plot:\n",
    "                plt.plot(x, y, c='tab:blue')\n",
    "        plt.title(f'Ramps for pixel {coords}')\n",
    "        plt.xlabel('Frame')\n",
    "\n",
    "        if save:\n",
    "            plt.savefig('temp_ramp_title.png',dpi=600,bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def plot_image(self,cube,frame,scale=None,save=False):\n",
    "        \"\"\"\n",
    "        just a quicker way to plot the images\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        plt.title(f'Frame {frame}')\n",
    "\n",
    "        if scale:\n",
    "            plt.imshow(cube[frame],vmin=scale[0],vmax=scale[1],origin='lower')\n",
    "        else:\n",
    "            plt.imshow(cube[frame],origin='lower')\n",
    "\n",
    "        plt.colorbar()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig('temp_img_title.png',dpi=600,bbox_inches='tight')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf078c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Jurassic('pipeline_data/Obs/stage1/jw02107011001_02107_00001_mirimage_ramp.fits')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jurassic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
